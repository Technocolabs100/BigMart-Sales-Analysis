# -*- coding: utf-8 -*-
"""BigMart mini priject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F3467jJiuo_54bmpmcpPs18FRxolKGhF

# `1.Stating the problem and problematic :`

The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim of this data science project is to build a predictive model and find out the sales of each product at a particular store.

# `2.Importing the relevant librairies:`
"""

from google.colab import files
uploaded = files.upload()
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LinearRegression

import warnings
warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

df_test =pd.read_csv("Test.csv")
df_test.head()

df_train =pd.read_csv("Train.csv")
df_train.head()

"""# `3.Data Inspection`

In order to define our dataset's structure (lines and columns , we will have to use df.shape())
"""

df_train .shape,df_test.shape

"""What we can understand is that We have 8523 rows and 12 columns in the Train set whereas Test set has 5681 rows and 11 columns. ( we have more data in the training set)

*After examining the Datasets , we can tell that the missing values belong to the Item_Weight and Outlet_size columns , but we still have to make sure of that by calculating the pourcentage of the missing data in both sets.*
"""

df_train.isnull().sum()/df_train.shape[0] *100

df_test.isnull().sum()/df_test.shape[0] *100

"""*Conclusion : It effectively appears that the Values are missing in the 2 columns :Item_Weight and Outlet_size . We can notice that the pourcentage of missing values is 17% for Item_Weight and 28% for Outlet_size , Later on we will procceed to deal with the missing data*

Now , before dealing with the missing data , we have different kind of items : Numerical and Categorical features, and we will deal with each one of them differently , let's have an idea of how many numerical & categorical items we have in our sets !
"""

#categorical features
categorical = df_train.select_dtypes(include =[np.object])
print("Categorical Features in Train Set:",categorical.shape[1])

#numerical features
numerical= df_train.select_dtypes(include =[np.float64,np.int64])
print("Numerical Features in Train Set:",numerical.shape[1])

#categorical features
categorical =df_test.select_dtypes(include =[np.object])
print("Categorical Features in Test Set:",categorical.shape[1])

#numerical features
numerical= df_test.select_dtypes(include =[np.float64,np.int64])
print("Numerical Features in Test Set:",numerical.shape[1])

"""*  In order to deal with the missing values , we will have to work with statistics , which means we have to calculate the count , mean, etc of the set , let's do that using the df.describe() function : *"""

df_train.describe()

df_test.describe()

"""# `4.Data cleaning and handling the missing values .`

As we already know Item_Weight and Outlet_Size got some missing values so in the coming parts we are giong to handle their misssing values

**4.1 Item weight :**  (numerical item)
"""

plt.figure(figsize=(7, 4))
sns.distplot(df_train['Item_Weight'], bins=20, kde=True)
plt.xlabel('Item Weight')
plt.ylabel('Density')
plt.title('Distplot of Item Weight')
plt.show()

plt.figure(figsize=(7, 4))
sns.distplot(df_test['Item_Weight'], bins=20, kde=True)
plt.xlabel('Item Weight')
plt.ylabel('Density')
plt.title('Distplot of Item Weight')
plt.show()

"""**The Distplots above clearly show no "Outliers" and so we can impute the missing values with "Mean"** : we will fill the missing values in the Item_Weight column with the mean of the non-missing values."""

df_train['Item_Weight']= df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean())
df_test['Item_Weight']= df_test['Item_Weight'].fillna(df_test['Item_Weight'].mean())

"""*Now let's test to see if there are any more missing values in this column*



"""

df_train['Item_Weight'].isnull().sum(),df_test['Item_Weight'].isnull().sum()

"""Apparently , there are no more missing values in this column , let's see :"""

df_test.head()

"""***We have succesfully imputed the missing values from the column
Item_Weight***

**4.2 Outlet size (categorical item):**
"""

print(df_train['Outlet_Size'].value_counts())
print('----------------------------------')
print(df_test['Outlet_Size'].value_counts())

"""*As we can notice , the Outlet size is a categorical item , it indicates whether the product is small,medium or high , in order to deal with categorical missing values, we will have to use the Mode (meaning the most repeated value)*"""

#Imputing with Mode
df_train['Outlet_Size']= df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0])
df_test['Outlet_Size']= df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0])

df_train['Outlet_Size'].isnull().sum(),df_test['Outlet_Size'].isnull().sum()

df_test.head()

"""**We have succesfully imputed the missing values from the column Outlet_Size.**

# `5.Data Visualisation and analysis`

**5.1 Univariate analysis:**

According to the dataset's columns , we will try to visualize criterions about items and then make assumptions and conclusions that will later on be of use.

Starting with the Item_type  column ,to indicate which kind of items are more available in stock.
"""

plt.figure(figsize=(25,7))
sns.countplot(y='Item_Type',data=df_train,palette='pastel')
plt.show()

"""From this plot we conclude that:

    Fruits and vegetables are largely available because they are used on a daily basis.

    Sea food is the least available.(not bought on daily basis)

Now moving to **the outlet size** column , to see which size is preferred by clients
"""

plt.figure(figsize=(5, 3))
sns.countplot(x='Outlet_Size', data=df_train, palette='plasma')

plt.show()

"""**People tend to go for medium outlets**

###**Outlet location type the most preferred:**
"""

# Get the count of each category in "Outlet_Location_Type" column
value_counts = df_train['Outlet_Location_Type'].value_counts()

colors = sns.color_palette('inferno')

# Plot the pie chart
plt.pie(value_counts, labels=value_counts.index, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Outlet Location Types')
plt.axis('equal')  # Equal aspect ratio ensures a circular pie chart

# Display the chart
plt.show()

"""**Oultes with location type tier 3 are the most preferred**

###**The bestselling product:**
"""

plt.figure(figsize=(8,6))
sns.barplot(y='Item_Type',x='Item_Outlet_Sales',data=df_train,palette='pastel')

plt.show()

"""

***The products available were Fruits-Veggies and Snack Foods but the sales of Seafood and Starchy Foods seems higher and hence the sales can be improved with having stock of products that are most bought by customers.***
"""

plt.figure(figsize=(5,2))
sns.barplot(y='Outlet_Type',x='Item_Outlet_Sales',data=df_train,palette='viridis')

plt.show()

"""***The sales are higher in The Supermarket Type3 , and they are smaller in outlets like Grocery stores. Which means we can focus on increasing the type 3 of supermarkets for example .***

**5.2. Bivariate analysis :**

*Let's test the correlation between different variables :*
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 4))
corr_matrix = df_train.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

plt.figure(figsize=(8, 4))
sns.pairplot(df_test, diag_kind='kde', markers='+')
plt.title('Pair Plot of Test Data')
plt.show()

"""After plotting this heatmap plot and the pair plot we can notice that the majority of our dataset variables have a weak correlation between each other except for the variables Item_MRP and Item_Outlet_Sales that have a relatively a stronger correlation but still not an enough reason for us to make a change in our dataset.

# `6.Feature engineering.`

***This is the part where we present our data in the best way possible in order to improve the performance of our upcoming model , we can do so by removing and adding features when needed.***

**6.1 Outlet Operation years:**


 One thing we can start with is the years of function of each Outlet , and we can easily calculate that by substracting the outlet establishment year from the present year (2023)
"""

#Years:
df_train['Outlet_function_Years'] = 2023 - df_train['Outlet_Establishment_Year']
df_test['Outlet_function_Years'] = 2023 - df_test['Outlet_Establishment_Year']

df_train.head()

"""**6.2 Item type ,simplified**

It's true that we already have each item's type in our item type column , but given that each type of item is defined within the 2 first letters of its identifier (for example FD stands for food, DR for drinks...) we can benefit from that in order to create a new catefory that we can call Item_type_new)
"""

df_train['Item_Type_new'] = df_train['Item_Identifier'].apply(lambda x: x[0:2])
df_train['Item_Type_new'] = df_train['Item_Type_new'].map({'FD':'Food',
                                                             'NC':'Non-Consumable',
                                                             'DR':'Drinks'})

df_test['Item_Type_new'] = df_test['Item_Identifier'].apply(lambda x: x[0:2])
df_test['Item_Type_new'] = df_test['Item_Type_new'].map({'FD':'Food',
                                                             'NC':'Non-Consumable',
                                                             'DR':'Drinks'})

df_train.head()

df_test.head()

"""**6.3 Modifying the Item_fat_content column :**

Now that we have set the new item type into 2 categories : food and non-consumable, we can modify the Item_fat_content column in order for it to resume the non-edible products' value into "non edible"
"""

#Mark non-consumables as separate category in low_fat:
df_train.loc[df_train['Item_Type_new']=="Non-Consumable",'Item_Fat_Content'] = "Non-Edible"
df_train['Item_Fat_Content'].value_counts()

#Mark non-consumables as separate category in low_fat:
df_test.loc[df_test['Item_Type_new']=="Non-Consumable",'Item_Fat_Content'] = "Non-Edible"
df_test['Item_Fat_Content'].value_counts()

"""***We can notice that t=here still is something wrong with this column , the 'Low fat' value and LF stand for the same thing , same goes for "Regular" and reg , let's merge them by replacing "Low fat" and "LF" with "Low fat" and "Regular" and "Reg" with "Regular"***"""

# Replace values in df_train
df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace(['LF', 'low fat'], 'Low Fat')
df_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].replace('reg', 'Regular')

# Replace values in df_test
df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].replace(['LF', 'low fat'], 'Low Fat')
df_test['Item_Fat_Content'] = df_test['Item_Fat_Content'].replace('reg', 'Regular')

df_train.head()

df_test.head()

"""# `7.Encoding categorical variables`

**This step is very crucial, we will have to convert categorical variable values into numerical ones , we will use both techniques  : Label encoding and one hot encoding :**

**Label encoding**
"""

#Train dataframe:

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
#New variable for outlet
df_train['Outlet'] = le.fit_transform(df_train['Outlet_Identifier'])
var_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_new','Outlet_Type','Outlet']
le = LabelEncoder()
for i in var_mod:
    df_train[i] = le.fit_transform(df_train[i])

#Test dataframe:

df_test['Outlet'] = le.fit_transform(df_test['Outlet_Identifier'])
var_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_new','Outlet_Type','Outlet']
le = LabelEncoder()
for i in var_mod:
    df_test[i] = le.fit_transform(df_test[i])

df_train.head()

"""Now let's use one hot encoding :

**One hot encoding :**
"""

#Train dataframe:

df_train = pd.get_dummies(df_train, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',
                              'Item_Type_new','Outlet'])

#Test dataframe:

df_test = pd.get_dummies(df_test, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',
                              'Item_Type_new','Outlet'])

df_test.head()

df_test.iloc[:, -20:]

"""# `8.Modeling :`

Now moving to the most important part , we will use linear regression in order to train our model and then test it to see if it works.

First of all , let's let go of some useless columns like : the identifiers
"""

df_train.columns

df_train = df_train.select_dtypes(exclude='object')
df_test2 = df_test.select_dtypes(exclude='object')
df_test2.head()

"""***Now ,let's separate the input variables(or features)from the output or target variables , in our case that would be the Item_Outlet_Sales.***"""

X= df_train.drop(columns = ['Item_Outlet_Sales'], axis=1)
Y= df_train['Item_Outlet_Sales']
X.head()

"""# ***Linear regression :***

**Training the model on the training dataset:**
"""

from sklearn.linear_model import LinearRegression

# Create an instance of the LinearRegression model
lr = LinearRegression()

# Train the model on the training data
lr.fit(X, Y)

"""**Model testing :**"""

# Predict the outputs for the test dataset
predictions = lr.predict(df_test2)
print(predictions)

"""**Merging the predicted sales with df_test dataframe:**"""

# Create a new dataframe 'df_predictions' with the predictions
df_predictions = pd.DataFrame(predictions, columns=['sales_predictions'])

# Merge the 'df_test' dataframe with the 'df_predictions' dataframe
df_merged = pd.concat([df_test, df_predictions], axis=1)
df_merged = df_merged[['Item_Identifier', 'Outlet_Identifier', 'sales_predictions']]
df_merged.head()

df_merged.to_csv('/content/sample_data/final.csv', index=False)
from google.colab import files
files.download('/content/sample_data/final.csv')

"""# **Regularized Linear Regression:**

Let's higher our model's performance level by adding regularized regression
"""

from sklearn.linear_model import Ridge, Lasso

# Regularized Linear Regression with Ridge (L2 regularization)
ridge = Ridge(alpha=0.5)  # Specify the regularization parameter alpha
ridge.fit(X, Y)  # Train the Ridge model on the training data

# Predict the outputs for the test dataset
predictions2 = lr.predict(df_test2)
print(predictions2)